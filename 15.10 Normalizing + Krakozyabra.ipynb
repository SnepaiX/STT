{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# !gdown \"https://drive.google.com/uc?id=1DiAXSK1DQf2beiN0-aJ3nDPK_f_5-S4C\"\n",
    "# !gdown \"https://drive.google.com/uc?id=1GRDrZjjOTfO5WohDX6Q83PgI5VMOyNlm\"\n",
    "# !gdown \"https://drive.google.com/uc?id=1FJhT_U4FHZaz5bosEAlfy-77dYn0qir5\"\n",
    "# !gdown \"https://drive.google.com/uc?id=1qEsOFMWxFZ4mSvEul2RuaY8JRq075bjZ\"\n",
    "# !gdown \"https://drive.google.com/uc?id=1Ko5WdTvaxME7XP2k6pNjttoc7dgL4I-x\"\n",
    "\n",
    "# !gdown \"https://drive.google.com/uc?id=1GRDrZjjOTfO5WohDX6Q83PgI5VMOyNlm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from string import printable, punctuation\n",
    "from tqdm import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install!!!\n",
    "class Normalizer:\n",
    "    def __init__(self,\n",
    "                 device='cpu',\n",
    "                 jit_model='jit_s2s.pt'):\n",
    "        super(Normalizer, self).__init__()\n",
    "\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        self.init_vocabs()\n",
    "\n",
    "        self.model = torch.jit.load(jit_model, map_location=device)\n",
    "        self.model.eval()\n",
    "        self.max_len = 150\n",
    "\n",
    "    def init_vocabs(self):\n",
    "        # Initializes source and target vocabularies\n",
    "\n",
    "        # vocabs\n",
    "        rus_letters = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ'\n",
    "        spec_symbols = '¼³№¾⅞½⅔⅓⅛⅜²'\n",
    "        # numbers + eng + punctuation + space + rus\n",
    "        self.src_vocab = {token: i + 5 for i, token in enumerate(printable[:-5] + rus_letters + '«»—' + spec_symbols)}\n",
    "        # punctuation + space + rus\n",
    "        self.tgt_vocab = {token: i + 5 for i, token in enumerate(punctuation + rus_letters + ' ' + '«»—')}\n",
    "\n",
    "        unk = '#UNK#'\n",
    "        pad = '#PAD#'\n",
    "        sos = '#SOS#'\n",
    "        eos = '#EOS#'\n",
    "        tfo = '#TFO#'\n",
    "        for i, token in enumerate([unk, pad, sos, eos, tfo]):\n",
    "            self.src_vocab[token] = i\n",
    "            self.tgt_vocab[token] = i\n",
    "\n",
    "        for i, token_name in enumerate(['unk', 'pad', 'sos', 'eos', 'tfo']):\n",
    "            setattr(self, '{}_index'.format(token_name), i)\n",
    "\n",
    "        inv_src_vocab = {v: k for k, v in self.src_vocab.items()}\n",
    "        self.src2tgt = {src_i: self.tgt_vocab.get(src_symb, -1) for src_i, src_symb in inv_src_vocab.items()}\n",
    "\n",
    "    def keep_unknown(self, string):\n",
    "        reg = re.compile(r'[^{}]+'.format(''.join(self.src_vocab.keys())))\n",
    "        unk_list = re.findall(reg, string)\n",
    "\n",
    "        unk_ids = [range(m.start() + 1, m.end()) for m in re.finditer(reg, string) if m.end() - m.start() > 1]\n",
    "        flat_unk_ids = [i for sublist in unk_ids for i in sublist]\n",
    "\n",
    "        upd_string = ''.join([s for i, s in enumerate(string) if i not in flat_unk_ids])\n",
    "        return upd_string, unk_list\n",
    "\n",
    "    def _norm_string(self, string):\n",
    "        # Normalizes chunk\n",
    "\n",
    "        if len(string) == 0:\n",
    "            return string\n",
    "        string, unk_list = self.keep_unknown(string)\n",
    "\n",
    "        token_src_list = [self.src_vocab.get(s, self.unk_index) for s in list(string)]\n",
    "        src = token_src_list + [self.eos_index] + [self.pad_index]\n",
    "\n",
    "        src2tgt = [self.src2tgt[s] for s in src]\n",
    "        src2tgt = torch.LongTensor(src2tgt).to(self.device)\n",
    "\n",
    "        src = torch.LongTensor(src).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            out = self.model(src, src2tgt)\n",
    "        pred_words = self.decode_words(out, unk_list)\n",
    "        if len(pred_words) > 199:\n",
    "            warnings.warn(\"Sentence {} is too long\".format(string), Warning)\n",
    "        return pred_words\n",
    "\n",
    "    def norm_text(self, text):\n",
    "        # Normalizes text\n",
    "\n",
    "        # Splits sentences to small chunks with weighted length <= max_len:\n",
    "        # * weighted length - estimated length of normalized sentence\n",
    "        #\n",
    "        # 1. Full text is splitted by \"ending\" symbols (\\n\\t?!.) to sentences;\n",
    "        # 2. Long sentences additionally splitted to chunks: by spaces or just dividing too long words\n",
    "\n",
    "        splitters = '\\n\\t?!'\n",
    "        parts = [p for p in re.split(r'({})'.format('|\\\\'.join(splitters)), text) if p != '']\n",
    "        norm_parts = []\n",
    "        for part in tqdm(parts):\n",
    "            if part in splitters:\n",
    "                norm_parts.append(part)\n",
    "            else:\n",
    "                weighted_string = [7 if symb.isdigit() else 1 for symb in part]\n",
    "                if sum(weighted_string) <= self.max_len:\n",
    "                    norm_parts.append(self._norm_string(part))\n",
    "                else:\n",
    "                    spaces = [m.start() for m in re.finditer(' ', part)]\n",
    "                    start_point = 0\n",
    "                    end_point = 0\n",
    "                    curr_point = 0\n",
    "\n",
    "                    while start_point < len(part):\n",
    "                        if curr_point in spaces:\n",
    "                            if sum(weighted_string[start_point:curr_point]) < self.max_len:\n",
    "                                end_point = curr_point + 1\n",
    "                            else:\n",
    "                                norm_parts.append(self._norm_string(part[start_point:end_point]))\n",
    "                                start_point = end_point\n",
    "\n",
    "                        elif sum(weighted_string[end_point:curr_point]) >= self.max_len:\n",
    "                            if end_point > start_point:\n",
    "                                norm_parts.append(self._norm_string(part[start_point:end_point]))\n",
    "                                start_point = end_point\n",
    "                            end_point = curr_point - 1\n",
    "                            norm_parts.append(self._norm_string(part[start_point:end_point]))\n",
    "                            start_point = end_point\n",
    "                        elif curr_point == len(part):\n",
    "                            norm_parts.append(self._norm_string(part[start_point:]))\n",
    "                            start_point = len(part)\n",
    "\n",
    "                        curr_point += 1\n",
    "        return ''.join(norm_parts)\n",
    "\n",
    "    def decode_words(self, pred, unk_list=None):\n",
    "        if unk_list is None:\n",
    "            unk_list = []\n",
    "        pred = pred.cpu().numpy()\n",
    "        pred_words = \"\".join(self.lookup_words(x=pred,\n",
    "                                               vocab={i: w for w, i in self.tgt_vocab.items()},\n",
    "                                               unk_list=unk_list))\n",
    "        return pred_words\n",
    "\n",
    "    def lookup_words(self, x, vocab, unk_list=None):\n",
    "        if unk_list is None:\n",
    "            unk_list = []\n",
    "        result = []\n",
    "        for i in x:\n",
    "            if i == self.unk_index:\n",
    "                if len(unk_list) > 0:\n",
    "                    result.append(unk_list.pop(0))\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                result.append(vocab[i])\n",
    "        return [str(t) for t in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "я уже две тысячи двенадцатый раз видел белок\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from normalizer import Normalizer\n",
    "# https://github.com/snakers4/russian_stt_text_normalization\n",
    "# https://towardsdatascience.com/russian-text-normalization-for-stt-and-tts-a6d8f03aaeb9\n",
    "\n",
    "text = 'я уже 2012 раз видел белок'\n",
    "\n",
    "norm = Normalizer()\n",
    "result = norm.norm_text(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os\n",
    "import IPython.display as ipd\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "\n",
    "for i in os.listdir():\n",
    "    os.chdir(i)\n",
    "    paths.append(i+'/transcription.csv')\n",
    "#     print(i)\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Radio1_50h', 'Radio2_25h', 'Radio3_25h', 'Radio5_25h']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paths = []\n",
    "paths = os.listdir()\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symboltt():\n",
    "# path_trans = \"Radio2_25h/Radio2_25h\"\n",
    "    os.chdir(\"dataset\")\n",
    "    paths = os.listdir()\n",
    "    name = \"transcription.csv\"\n",
    "\n",
    "    for i in range(len(paths)):\n",
    "        metadata = pd.read_csv(paths[i]+'/'+name)\n",
    "        data_to_text = paths[i]+'/'+\"data.csv\"\n",
    "\n",
    "        with open(data_to_text, 'w') as f:\n",
    "            for index, row in metadata.iterrows():\n",
    "                f.write(str(row['duration']) + ',' + str(row['filepath']) + ',' + str(row['text']) + '\\n')\n",
    "        print('end')\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = 'data.csv'\n",
    "c = 0 \n",
    "for i in range(len(paths)):\n",
    "    with open(os.listdir()[i]+ '/' + data, 'r') as f:\n",
    "        for row in f:\n",
    "            c += 1\n",
    "            temp = row[0:-1].split(',')\n",
    "#             print(temp)\n",
    "            dur =  temp[0]\n",
    "            filename = temp[1]\n",
    "            trans = temp[2]\n",
    "#             if c == 1395:\n",
    "#                 break\n",
    "#             print(filename)\n",
    "            nums = '0123456789'\n",
    "            for i in nums:\n",
    "                if i in trans:\n",
    "                    print(trans)\n",
    "#     ipd.Audio(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfccs(path):\n",
    "    y, sr = librosa.load(path)\n",
    "    mfcc = librosa.feature.mfcc(y, sr, n_mfcc=40)\n",
    "    return mfcc\n",
    "\n",
    "# def extense(features, frames_max):\n",
    "#     extensed = []\n",
    "#     for i in range(len(features)):\n",
    "#         if (len(features[0]) < frames_max):\n",
    "#             total = frames_max - len(features[0])\n",
    "#             left = total//2\n",
    "#             right = total-left\n",
    "#             features[i] = np.pad(features[i], pad_width=((0,0), (left, right)), mode='constant')\n",
    "#         extensed.append(features[i])\n",
    "#     return extensed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel = get_mfccs(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'librosa' has no attribute 'display'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12060/3383181791.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'librosa' has no attribute 'display'"
     ]
    }
   ],
   "source": [
    "librosa.display.specshow(mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 10, 27, 17, 59, 37, 728338)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import librosa.display\n",
    "from datetime import datetime\n",
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "def b():\n",
    "    s = \"абвгдеёжзийклмнопрстуфхцчшщъыьэюя\"\n",
    "    for i in range(len(s)):\n",
    "        print(datetime.now())\n",
    "        print(i)\n",
    "\n",
    "def c():\n",
    "    s = \"абвгдеёжзийклмнопрстуфхцчшщъыьэюя\"\n",
    "    for i in range(len(s)):\n",
    "        print(datetime.now())\n",
    "        print(s[i])\n",
    "\n",
    "\n",
    "thread1 = Process(target = b)\n",
    "thread2 = Process(target = c)\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "def prescript(thefile, num):\n",
    "    with open(thefile, 'w') as f:\n",
    "        for i in range(num):\n",
    "            if num > 500:\n",
    "                f.write('МногоБукв\\n')\n",
    "            else:\n",
    "                f.write('МалоБукв\\n')\n",
    "\n",
    "thread1 = Thread(target=prescript, args=('f1.txt', 200,))\n",
    "thread2 = Thread(target=prescript, args=('f2.txt', 1000,))\n",
    "\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "thread1.join()\n",
    "thread2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working class A, i=0\n",
      "Working class A, i=1\n",
      "Working class A, i=2\n",
      "Working class A, i=3\n",
      "Working class A, i=4\n",
      "Working class A, i=5\n",
      "Working class A, i=6\n",
      "Working class A, i=7\n",
      "Working class A, i=8\n",
      "Working class A, i=9\n",
      "Working class B, i=0\n",
      "Working class B, i=1\n",
      "Working class B, i=2\n",
      "Working class B, i=3\n",
      "Working class B, i=4\n",
      "Working class B, i=5\n",
      "Working class B, i=6\n",
      "Working class B, i=7\n",
      "Working class B, i=8\n",
      "Working class B, i=9\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process\n",
    "from time import sleep\n",
    "\n",
    "class A:\n",
    "    def call(self, count=10, sleep_time=0.5):\n",
    "        for i in range(count):\n",
    "            print('Working class A, i=%s' % i)\n",
    "            sleep(sleep_time)\n",
    "\n",
    "\n",
    "class B:\n",
    "    def call(self, count=10, sleep_time=0.5):\n",
    "        for i in range(count):\n",
    "            print('Working class B, i=%s' % i)\n",
    "            sleep(sleep_time)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    a = A().call()\n",
    "    b = B().call()\n",
    "\n",
    "    p1 = Process(target=a, kwargs={'sleep_time': 0.7})\n",
    "    p2 = Process(target=b, args=(12,))\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "\n",
    "    p1.join()\n",
    "    p2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working class A, i=0\n",
      "Working class B, i=0\n",
      "Working class B, i=1\n",
      "Working class B, i=2\n",
      "Working class B, i=3\n",
      "Working class B, i=4\n",
      "Working class B, i=5\n",
      "Working class B, i=6\n",
      "Working class B, i=7\n",
      "Working class B, i=8\n",
      "Working class B, i=9\n",
      "Working class B, i=10\n",
      "Working class B, i=11\n",
      "Working class A, i=1\n",
      "Working class A, i=2\n",
      "Working class A, i=3\n",
      "Working class A, i=4\n",
      "Working class A, i=5\n",
      "Working class A, i=6\n",
      "Working class A, i=7\n",
      "Working class A, i=8\n",
      "Working class A, i=9\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "class A:\n",
    "    def __call__(self, count=10, sleep_time=0.5):\n",
    "        for i in range(count):\n",
    "            print('Working class A, i=%s' % i)\n",
    "            sleep(sleep_time)\n",
    "\n",
    "\n",
    "class B:\n",
    "    def __call__(self, count=10, sleep_time=0.2):\n",
    "        for i in range(count):\n",
    "            x = 10\n",
    "#             for j in range(10):\n",
    "                       # какая-то долгая операция\n",
    "            print('Working class B, i=%s' % i)\n",
    "if __name__ == '__main__':\n",
    "    a = A()\n",
    "    b = B()\n",
    "\n",
    "    t1 = threading.Thread(target=a, kwargs={'sleep_time': 0.1})\n",
    "    t2 = threading.Thread(target=b, args=(12,))\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "\n",
    "    t1.join()\n",
    "    t2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001E00F7CB3D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pyaudio/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001E00F7B27F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pyaudio/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001E00F7B2880>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pyaudio/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001E00F7CB610>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pyaudio/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001E00F7CB7C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pyaudio/\n",
      "ERROR: Could not find a version that satisfies the requirement pyaudio (from versions: none)\n",
      "ERROR: No matching distribution found for pyaudio\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
